{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synsets('dog')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dog', u'domestic_dog', u'Canis_familiaris']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def lemmatize(syn):\n",
    "    return syn.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'canine', u'canid'], [u'domestic_animal', u'domesticated_animal']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[syn.lemma_names() for syn in dog.hypernyms()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'basenji',\n",
       " u'corgi',\n",
       " u'Welsh_corgi',\n",
       " u'cur',\n",
       " u'mongrel',\n",
       " u'mutt',\n",
       " u'dalmatian',\n",
       " u'coach_dog',\n",
       " u'carriage_dog',\n",
       " u'Great_Pyrenees',\n",
       " u'griffon',\n",
       " u'Brussels_griffon',\n",
       " u'Belgian_griffon',\n",
       " u'hunting_dog',\n",
       " u'lapdog',\n",
       " u'Leonberg',\n",
       " u'Mexican_hairless',\n",
       " u'Newfoundland',\n",
       " u'Newfoundland_dog',\n",
       " u'pooch',\n",
       " u'doggie',\n",
       " u'doggy',\n",
       " u'barker',\n",
       " u'bow-wow',\n",
       " u'poodle',\n",
       " u'poodle_dog',\n",
       " u'pug',\n",
       " u'pug-dog',\n",
       " u'puppy',\n",
       " u'spitz',\n",
       " u'toy_dog',\n",
       " u'toy',\n",
       " u'working_dog']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([lemmatize(syn) for syn in dog.hyponyms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('canis.n.01'), Synset('pack.n.06')]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print dog.member_holonyms()\n",
    "print dog.substance_holonyms()\n",
    "print dog.part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[Synset('flag.n.07')]\n"
     ]
    }
   ],
   "source": [
    "print dog.member_meronyms()\n",
    "print dog.substance_meronyms()\n",
    "print dog.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print dog.similar_tos()\n",
    "print dog.topic_domains()\n",
    "print dog.region_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL related wordnet words\n",
    "\n",
    "def get_related_wordnet_lemmas(syn):\n",
    "    '''\n",
    "    Given a synset 'syn', return all lemmas of that synset, as well as all lemmas of *related* synsets\n",
    "    '''  \n",
    "\n",
    "    relatedness_methods = ['hypernyms','instance_hypernyms','hyponyms','instance_hyponyms','member_holonyms','substance_holonyms','part_holonyms','member_meronyms','substance_meronyms','part_meronyms','topic_domains','region_domains','usage_domains','attributes','entailments','causes','also_sees','verb_groups','similar_tos']\n",
    "    \n",
    "    lemmas = set(lemmatize(syn))\n",
    "    for method in relatedness_methods:\n",
    "        related_synsets = getattr(syn, method)()\n",
    "        lemmas.update(flatten([lemmatize(s) for s in related_synsets]))\n",
    "        \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Belgian_griffon',\n",
       " u'Brussels_griffon',\n",
       " u'Canis',\n",
       " u'Canis_familiaris',\n",
       " u'Great_Pyrenees',\n",
       " u'Leonberg',\n",
       " u'Mexican_hairless',\n",
       " u'Newfoundland',\n",
       " u'Newfoundland_dog',\n",
       " u'Welsh_corgi',\n",
       " u'barker',\n",
       " u'basenji',\n",
       " u'bow-wow',\n",
       " u'canid',\n",
       " u'canine',\n",
       " u'carriage_dog',\n",
       " u'coach_dog',\n",
       " u'corgi',\n",
       " u'cur',\n",
       " u'dalmatian',\n",
       " u'dog',\n",
       " u'doggie',\n",
       " u'doggy',\n",
       " u'domestic_animal',\n",
       " u'domestic_dog',\n",
       " u'domesticated_animal',\n",
       " u'flag',\n",
       " u'genus_Canis',\n",
       " u'griffon',\n",
       " u'hunting_dog',\n",
       " u'lapdog',\n",
       " u'mongrel',\n",
       " u'mutt',\n",
       " u'pack',\n",
       " u'pooch',\n",
       " u'poodle',\n",
       " u'poodle_dog',\n",
       " u'pug',\n",
       " u'pug-dog',\n",
       " u'puppy',\n",
       " u'spitz',\n",
       " u'toy',\n",
       " u'toy_dog',\n",
       " u'working_dog'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_wordnet_lemmas(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bittersweet',\n",
       " u'doleful',\n",
       " u'heavyhearted',\n",
       " u'melancholic',\n",
       " u'melancholy',\n",
       " u'mournful',\n",
       " u'pensive',\n",
       " u'sad',\n",
       " u'tragic',\n",
       " u'tragical',\n",
       " u'tragicomic',\n",
       " u'tragicomical',\n",
       " u'wistful'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad = wn.synsets('sad')[0]\n",
    "get_related_wordnet_lemmas(sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'empty',\n",
       " u'empty-bellied',\n",
       " u'esurient',\n",
       " u'famished',\n",
       " u'hungry',\n",
       " u'peckish',\n",
       " u'ravenous',\n",
       " u'sharp-set',\n",
       " u'starved',\n",
       " u'supperless'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad = wn.synsets('hungry')[0]\n",
    "get_related_wordnet_lemmas(sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bogeyman',\n",
       " u'booger',\n",
       " u'boogeyman',\n",
       " u'bugaboo',\n",
       " u'bugbear',\n",
       " u'imaginary_being',\n",
       " u'imaginary_creature',\n",
       " u'monster',\n",
       " u'mythical_creature',\n",
       " u'mythical_monster'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monster = wn.synsets('monster')[0]\n",
    "get_related_wordnet_lemmas(monster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Ceratotherium_simum',\n",
       " u'Diceros_bicornis',\n",
       " u'Diceros_simus',\n",
       " u'Indian_rhinoceros',\n",
       " u'Rhinoceros_antiquitatis',\n",
       " u'Rhinoceros_unicornis',\n",
       " u'Rhinocerotidae',\n",
       " u'black_rhinoceros',\n",
       " u'family_Rhinocerotidae',\n",
       " u'odd-toed_ungulate',\n",
       " u'perissodactyl',\n",
       " u'perissodactyl_mammal',\n",
       " u'rhino',\n",
       " u'rhinoceros',\n",
       " u'rhinoceros_family',\n",
       " u'white_rhinoceros',\n",
       " u'woolly_rhinoceros'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhino = wn.synsets('rhino')[0]\n",
    "get_related_wordnet_lemmas(rhino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Ritz',\n",
       " u'auberge',\n",
       " u'building',\n",
       " u'court',\n",
       " u'edifice',\n",
       " u'fleabag',\n",
       " u'holiday_resort',\n",
       " u'hostel',\n",
       " u'hostelry',\n",
       " u'hotel',\n",
       " u'hotel_room',\n",
       " u'inn',\n",
       " u'lodge',\n",
       " u'motor_hotel',\n",
       " u'motor_inn',\n",
       " u'motor_lodge',\n",
       " u'resort',\n",
       " u'resort_hotel',\n",
       " u'ski_lodge',\n",
       " u'spa',\n",
       " u'tourist_court'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel = wn.synsets('hotel')[0]\n",
    "get_related_wordnet_lemmas(hotel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'empty',\n",
       " u'empty-bellied',\n",
       " u'esurient',\n",
       " u'famished',\n",
       " u'hungry',\n",
       " u'peckish',\n",
       " u'ravenous',\n",
       " u'sharp-set',\n",
       " u'starved',\n",
       " u'supperless'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hungry = wn.synsets('hungry')[0]\n",
    "get_related_wordnet_lemmas(hungry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Eames_chair',\n",
       " u'armchair',\n",
       " u'back',\n",
       " u'backrest',\n",
       " u'barber_chair',\n",
       " u'chair',\n",
       " u'chair_of_state',\n",
       " u'chaise',\n",
       " u'chaise_longue',\n",
       " u'daybed',\n",
       " u'feeding_chair',\n",
       " u'fighting_chair',\n",
       " u'folding_chair',\n",
       " u'garden_chair',\n",
       " u'highchair',\n",
       " u'ladder-back',\n",
       " u'ladder-back_chair',\n",
       " u'lawn_chair',\n",
       " u'leg',\n",
       " u'rocker',\n",
       " u'rocking_chair',\n",
       " u'seat',\n",
       " u'side_chair',\n",
       " u'straight_chair',\n",
       " u'swivel_chair',\n",
       " u'tablet-armed_chair',\n",
       " u'wheelchair'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chair = wn.synsets('chair')[0]\n",
    "get_related_wordnet_lemmas(chair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Iosif_Vissarionovich_Dzhugashvili',\n",
       " u'Joseph_Stalin',\n",
       " u'Stalin',\n",
       " u'commie',\n",
       " u'communist'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stalin = wn.synsets('Stalin')[0]\n",
    "get_related_wordnet_lemmas(Stalin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'accelerator',\n",
       " u'accelerator_pedal',\n",
       " u'aeroplane',\n",
       " u'airliner',\n",
       " u'airplane',\n",
       " u'amphibian',\n",
       " u'amphibious_aircraft',\n",
       " u'attack_aircraft',\n",
       " u'biplane',\n",
       " u'bomber',\n",
       " u'bonnet',\n",
       " u'cowl',\n",
       " u'cowling',\n",
       " u'delta_wing',\n",
       " u'escape_hatch',\n",
       " u'fighter',\n",
       " u'fighter_aircraft',\n",
       " u'fuel_pod',\n",
       " u'fuselage',\n",
       " u'gas',\n",
       " u'gas_pedal',\n",
       " u'gun',\n",
       " u'hangar_queen',\n",
       " u'heavier-than-air_craft',\n",
       " u'hood',\n",
       " u'hydroplane',\n",
       " u'jet',\n",
       " u'jet-propelled_plane',\n",
       " u'jet_plane',\n",
       " u'landing_gear',\n",
       " u'monoplane',\n",
       " u'multiengine_airplane',\n",
       " u'multiengine_plane',\n",
       " u'navigation_light',\n",
       " u'plane',\n",
       " u'pod',\n",
       " u'propeller_plane',\n",
       " u'radar_dome',\n",
       " u'radome',\n",
       " u'reconnaissance_plane',\n",
       " u'seaplane',\n",
       " u'ski-plane',\n",
       " u'tanker_plane',\n",
       " u'throttle',\n",
       " u'windscreen',\n",
       " u'windshield',\n",
       " u'wing'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airplane = wn.synsets('airplane')[0]\n",
    "get_related_wordnet_lemmas(airplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'attendant', u'attender', u'batman', u'tender'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batman = wn.synsets('Batman')[0]\n",
    "get_related_wordnet_lemmas(Batman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'aggravated',\n",
       " u'angered',\n",
       " u'angry',\n",
       " u'black',\n",
       " u'choleric',\n",
       " u'enraged',\n",
       " u'furious',\n",
       " u'hot_under_the_collar',\n",
       " u'huffy',\n",
       " u'incensed',\n",
       " u'indignant',\n",
       " u'infuriated',\n",
       " u'irascible',\n",
       " u'irate',\n",
       " u'ireful',\n",
       " u'livid',\n",
       " u'mad',\n",
       " u'maddened',\n",
       " u'outraged',\n",
       " u'provoked',\n",
       " u'smoldering',\n",
       " u'smouldering',\n",
       " u'sore',\n",
       " u'umbrageous',\n",
       " u'wrathful',\n",
       " u'wroth',\n",
       " u'wrothful'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angry = wn.synsets('angry')[0]\n",
    "get_related_wordnet_lemmas(angry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'black_comedy',\n",
       " u'comedy',\n",
       " u\"commedia_dell'arte\",\n",
       " u'dark_comedy',\n",
       " u'drama',\n",
       " u'farce',\n",
       " u'farce_comedy',\n",
       " u'high_comedy',\n",
       " u'low_comedy',\n",
       " u'melodrama',\n",
       " u'seriocomedy',\n",
       " u'sitcom',\n",
       " u'situation_comedy',\n",
       " u'slapstick',\n",
       " u'tragicomedy',\n",
       " u'travesty'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedy = wn.synsets('comedy')[0]\n",
    "get_related_wordnet_lemmas(comedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Jack_the_Ripper',\n",
       " u'assassin',\n",
       " u'assassinator',\n",
       " u'bravo',\n",
       " u'butcher',\n",
       " u'criminal',\n",
       " u'crook',\n",
       " u'cutthroat',\n",
       " u'felon',\n",
       " u'fratricide',\n",
       " u'gun',\n",
       " u'gun_for_hire',\n",
       " u'gunman',\n",
       " u'gunslinger',\n",
       " u'hatchet_man',\n",
       " u'hired_gun',\n",
       " u'hit_man',\n",
       " u'hitman',\n",
       " u'iceman',\n",
       " u'infanticide',\n",
       " u'killer',\n",
       " u'liquidator',\n",
       " u'malefactor',\n",
       " u'manslayer',\n",
       " u'mass_murderer',\n",
       " u'murderer',\n",
       " u'murderess',\n",
       " u'outlaw',\n",
       " u'parricide',\n",
       " u'ripper',\n",
       " u'serial_killer',\n",
       " u'serial_murderer',\n",
       " u'shooter',\n",
       " u'slayer',\n",
       " u'torpedo',\n",
       " u'triggerman'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murderer = wn.synsets('murderer')[0]\n",
    "get_related_wordnet_lemmas(murderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dOg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'spy', u'undercover_agent']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('spy')[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'spy', u'undercover_agent']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('spies')[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'descry', u'spot', u'espy', u'spy']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('spied')[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'spying']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('spying')[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('tailor.n.01'),\n",
       " Synset('tailor.v.01'),\n",
       " Synset('cut.v.07'),\n",
       " Synset('sew.v.02')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('tailor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_rhinoceros 0\n",
      "Diceros_bicornis 0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets('black_rhinoceros')\n",
    "for s in syns:\n",
    "    for l in s.lemmas():\n",
    "        print str(l.name()) + \" \" + str(l.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('tailor.n.01.tailor')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'tailor'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lmt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smoldering'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('smoldering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smouldering'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('smouldering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'superduperuglyduckling'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('superduperuglyduckling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckling'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('duckling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'duck'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('ducks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'melancholic', u'bittersweet', u'melancholy', u'pensive', u'tragical', u'heavyhearted', u'sad', u'tragic', u'wistful', u'tragicomic', u'mournful', u'doleful', u'tragicomical'])\n",
      "set([u'sorrowful', u'sad'])\n",
      "set([u'distressing', u'sad', u'bad', u'pitiful', u'lamentable', u'deplorable', u'sorry'])\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('sad'):\n",
    "    print get_related_wordnet_lemmas(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('sad.a.01'), Synset('sad.s.02'), Synset('deplorable.s.01')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'armed_forces', u'spy', u'armed_services', u'infiltrator', u'espionage_agent', u'intelligence_agent', u'secret_agent', u'counterspy', u'double_agent', u'foreign_agent', u'military_machine', u'war_machine', u'intelligence_officer', u'sleeper', u'undercover_agent', u'Margarete_Gertrud_Zelle', u'military', u'mole', u'operative', u'Mata_Hari'])\n",
      "set([u'spy', u'snoop', u'looker', u'viewer', u'shadower', u'watcher', u'snooper', u'tail', u'spectator', u'shadow', u'witness'])\n",
      "set([u'espy', u'spot', u'sight', u'descry', u'spy'])\n",
      "set([u'sleuth', u'monitor', u'snoop', u'spy', u'inquire', u'stag', u'investigate', u'enquire', u'supervise'])\n",
      "set([u'spy', u'notice', u'detect', u'sight', u'spot', u'perceive', u'comprehend', u'discover', u'espy', u'observe', u'descry', u'find'])\n",
      "set([u'inquire', u'spy', u'investigate', u'enquire'])\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('spy'):\n",
    "    print get_related_wordnet_lemmas(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spied'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('spied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'spy'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('spied', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('impropriety.n.01'),\n",
       " Synset('impropriety.n.02'),\n",
       " Synset('indecency.n.02'),\n",
       " Synset('familiarity.n.05')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('impROPriety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'impropriety'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmt.lemmatize('impropriety', 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peddler\n",
      "peddler\n",
      "peddler\n",
      "peddler\n",
      "peddler\n"
     ]
    }
   ],
   "source": [
    "for pos in ['n','v','a','s','r']:\n",
    "    print lmt.lemmatize('peddler', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('peddler.n.01'), Synset('pusher.n.02')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('peddler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_wordnet_lemmas(grapheme):\n",
    "    '''\n",
    "    Return *all* lemmas for *all* synsets within distance-1 of *all* of the grapheme's synsets\n",
    "    Don't need to muck around with cases, because WordNet is smart enough to handle that itself\n",
    "    '''\n",
    "    if not wn.synsets(grapheme):\n",
    "        return []\n",
    "    \n",
    "    # These are all of the non-antonym non-POS synset relationships recognized by WordNet\n",
    "    relationship_types = [\n",
    "        'hypernyms','instance_hypernyms','hyponyms','instance_hyponyms', \\\n",
    "        'member_holonyms','substance_holonyms','part_holonyms','member_meronyms', \\\n",
    "        'substance_meronyms','part_meronyms','topic_domains','region_domains', \\\n",
    "        'usage_domains','attributes','entailments','causes','also_sees','verb_groups','similar_tos']\n",
    "    \n",
    "    lemmas = set()\n",
    "    for synset in wn.synsets(grapheme):\n",
    "        # distance-0 lemmas\n",
    "        lemmas.update(synset.lemma_names())\n",
    "        # distance-1 lemmas\n",
    "        for relationship in relationship_types:\n",
    "            related_synsets = getattr(synset, relationship)()\n",
    "            lemmas.update(flatten([s.lemma_names() for s in related_synsets]))\n",
    "        \n",
    "    # Convert from unicode with str, and return the results as a list\n",
    "    return list(map(str, lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trafficker',\n",
       " 'pedlar',\n",
       " 'cosmetician',\n",
       " 'dealer',\n",
       " 'peddler',\n",
       " 'ticket_agent',\n",
       " 'seller',\n",
       " 'booking_clerk',\n",
       " 'cheap-jack',\n",
       " 'vender',\n",
       " 'selling_agent',\n",
       " 'marketer',\n",
       " 'merchant',\n",
       " 'merchandiser',\n",
       " 'vendor',\n",
       " 'pitchman',\n",
       " 'underseller',\n",
       " 'hawker',\n",
       " 'flower_girl',\n",
       " 'fruiterer',\n",
       " 'packman',\n",
       " 'huckster']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = get_related_wordnet_lemmas('seller')\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_shortest_lemma(grapheme, lemmatizer=WordNetLemmatizer()):\n",
    "    '''\n",
    "    Need to check all possible parts of parts of speech to make sure that we identify the *shortest* lemma\n",
    "    If don't manually check all parts of speech, it defaults to using the lemmas of the first synset, which may\n",
    "    not be what we want.\n",
    "\n",
    "    The parts of speech are:\n",
    "      n : NOUN\n",
    "      v : VERB\n",
    "      a : ADJECTIVE\n",
    "      s : ADJECTIVE SATELLITE\n",
    "      r : ADVERB\n",
    "\n",
    "    Note: be sure to predefine/pass in the lemmatizer in advance, so that it doesn't need to be recreated on each run\n",
    "    '''\n",
    "    shortest_lemma = grapheme\n",
    "    for pos in ['n','v','a','s','r']:\n",
    "        lemma = lemmatizer.lemmatize(grapheme, pos)\n",
    "        if len(lemma) < len(shortest_lemma):\n",
    "            shortest_lemma = lemma\n",
    "    return shortest_lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trafficker trafficker\n",
      "pedlar pedlar\n",
      "cosmetician cosmetician\n",
      "dealer dealer\n",
      "peddler peddler\n",
      "ticket_agent ticket_agent\n",
      "seller seller\n",
      "booking_clerk booking_clerk\n",
      "cheap-jack cheap-jack\n",
      "vender vender\n",
      "selling_agent selling_agent\n",
      "marketer marketer\n",
      "merchant merchant\n",
      "merchandiser merchandiser\n",
      "vendor vendor\n",
      "pitchman pitchman\n",
      "underseller underseller\n",
      "hawker hawker\n",
      "flower_girl flower_girl\n",
      "fruiterer fruiterer\n",
      "packman packman\n",
      "huckster huckster\n"
     ]
    }
   ],
   "source": [
    "gs = get_related_wordnet_lemmas('seller')\n",
    "for g in gs:\n",
    "    print g, get_shortest_lemma(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'spy'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shortest_lemma('spied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melancholic melancholic \n",
      "bittersweet bittersweet bittersweet\n",
      "melancholy melancholy \n",
      "pensive pensive \n",
      "tragical tragical tragic\n",
      "heavyhearted heavyhearted \n",
      "sorrowful sorrowful sorrow\n",
      "bad bad bad\n",
      "sad sad sad\n",
      "deplorable deplorable \n",
      "distressing distress distress\n",
      "sorry sorry \n",
      "tragic tragic tragic\n",
      "wistful wistful \n",
      "tragicomic tragicomic \n",
      "mournful mournful mourn\n",
      "lamentable lamentable lament\n",
      "doleful doleful dole\n",
      "pitiful pitiful \n",
      "tragicomical tragicomical \n"
     ]
    }
   ],
   "source": [
    "gr = get_related_wordnet_lemmas('sad')\n",
    "for g in gr:\n",
    "    gl = get_shortest_lemma(g)\n",
    "    gs = porter_stemmer.stem(gl)\n",
    "    if not wn.synsets(gs):\n",
    "        gs = ''\n",
    "    print g, gl, gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is ONLY valid if the stem is present in wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
