{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every word in the CMU-pronunciation dict:\n",
    "1. remove word if it contains characters other than :alpha, :hyphen, :underscore\n",
    "2. associate with its first phoneme match, with stresses stripped\n",
    "3. contruct input file of the form: `\"letter_1\\sletter_2\\s...letter_n\\tphone_1\\sphone_2\\s...phone_m\"`\n",
    "4. call m2m-aligner on file (https://github.com/letter-to-phoneme/m2m-aligner)\n",
    "5. process results into data structure that allows for easy portmanteau generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123455"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "cmu_dict = nltk.corpus.cmudict.dict()\n",
    "\n",
    "len(cmu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"^([a-z_\\-]+)+$\")\n",
    "\n",
    "# took ~6sec to run ~1k pairs, therefore to run ~100k pairs will take ~600sec = 10min\n",
    "with open('../data/m2m_preprocessed_cmudict.txt', 'w') as outfile:\n",
    "    for word, phoneme_list in cmu_dict.iteritems():\n",
    "        # don't need to downcase, because all words in cmudict are already downcased\n",
    "        # only allow words comprised of:\n",
    "        # 1) alpha\n",
    "        # 2) hyphen '-'\n",
    "        # 3) underscore '_'\n",
    "        if pattern.match(word):\n",
    "            clean_phoneme = [filter(str.isalpha, str(phone)) for phone in phoneme_list[0]]\n",
    "            outfile.write(' '.join(word) + '\\t' + ' '.join(clean_phoneme) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jr\n",
      "mr\n",
      "ltd\n",
      "st\n",
      "se\n",
      "dwi\n",
      "etc\n",
      "feb\n",
      "nov\n",
      "cxc\n",
      "dfw\n",
      "wm\n",
      "kwh\n",
      "aaa\n",
      "vs\n",
      "w\n",
      "x\n",
      "q\n",
      "sr\n",
      "ws\n",
      "bbq\n"
     ]
    }
   ],
   "source": [
    "# run m2m-aligner in command line:\n",
    "# > PATH/TO/M2M_ALIGNER/m2m-aligner --delX --maxX 2 --maxY 2 -i data/m2m_preprocessed_cmudict.txt\n",
    "\n",
    "# failed to align 21 words out of the total set of 100k, I'd say that's pretty good\n",
    "# especially considering that the failures are extreme edge-cases of written english\n",
    "\n",
    "with open('../data/m2m_preprocessed_cmudict.txt.m-mAlign.2-2.delX.1-best.conYX.align.err') as infile:\n",
    "    failed_words = [line.strip().split('\\t')[0].replace(' ','') for line in infile.readlines()]\n",
    "\n",
    "print '\\n'.join(failed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligned pairs use the following syntax:\n",
    "# 1) chunked graphemes/phonemes are divided by '|' symbols\n",
    "# 2) two graphemes/phonemes which are chunked together in a mapping will be separated by a ':'\n",
    "# 3) graphemes mapping to null-phonemes are denoted by '_'\n",
    "\n",
    "with open('../data/m2m_preprocessed_cmudict.txt.m-mAlign.2-2.delX.1-best.conYX.align') as infile:\n",
    "    aligned_word_phoneme_pairs = [line.strip().split('\\t') for line in infile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impelled\n",
      "[('i',), ('m',), ('p',), ('e',), ('l', 'l'), ('e',), ('d',)]\n",
      "[('IH',), ('M',), ('P',), ('EH',), ('L',), (), ('D',)]\n"
     ]
    }
   ],
   "source": [
    "seq1 = 'i|m|p|e|l:l|e|d|'\n",
    "seq2 = 'IH|M|P|EH|L|_|D|'\n",
    "\n",
    "divider_char = '|'\n",
    "concat_char = ':'\n",
    "null_char = '_' # null char CAN appear validly in seq1, and never appears with its usual usage\n",
    "\n",
    "hash_key = seq1.replace(divider_char,'').replace(concat_char,'')\n",
    "\n",
    "# get sequence tuples\n",
    "\n",
    "seq1_chunks = seq1.strip(divider_char).split(divider_char)\n",
    "new_seq1 = []\n",
    "for chunk in seq1_chunks:\n",
    "    # do NOT filter out null_chars\n",
    "    new_chunk = tuple(chunk.split(concat_char))\n",
    "    new_seq1.append(new_chunk)\n",
    "\n",
    "seq2_chunks = seq2.strip(divider_char).split(divider_char)\n",
    "new_seq2 = []\n",
    "for chunk in seq2_chunks:\n",
    "    # DO filter out null_chars\n",
    "    if chunk == null_char:\n",
    "        new_chunk = ()\n",
    "    else:\n",
    "        new_chunk = tuple(chunk.split(concat_char))\n",
    "    new_seq2.append(new_chunk)\n",
    "    \n",
    "print hash_key\n",
    "print new_seq1\n",
    "print new_seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess alignment results into hash mapping grapheme to 2 lists-of-tuples\n",
    "divider_char = '|'\n",
    "concat_char = ':'\n",
    "null_char = '_' # null char CAN appear validly in seq1, and never appears with its usual usage\n",
    "\n",
    "grapheme_phoneme_alignment_dict = {}\n",
    "\n",
    "for seq1, seq2 in aligned_word_phoneme_pairs:\n",
    "    # get seq1 hash key\n",
    "    hash_key = seq1.replace(divider_char,'').replace(concat_char,'')\n",
    "    \n",
    "    # get seq1 tuples\n",
    "    seq1_chunks = seq1.strip(divider_char).split(divider_char)\n",
    "    new_seq1 = []\n",
    "    for chunk in seq1_chunks:\n",
    "        # do NOT filter out null_chars\n",
    "        new_chunk = tuple(chunk.split(concat_char))\n",
    "        new_seq1.append(new_chunk)\n",
    "\n",
    "    # get seq2 tuples\n",
    "    seq2_chunks = seq2.strip(divider_char).split(divider_char)\n",
    "    new_seq2 = []\n",
    "    for chunk in seq2_chunks:\n",
    "        # DO filter out null_chars\n",
    "        if chunk == null_char:\n",
    "            new_chunk = ()\n",
    "        else:\n",
    "            new_chunk = tuple(chunk.split(concat_char))\n",
    "        new_seq2.append(new_chunk)\n",
    "    \n",
    "    grapheme_phoneme_alignment_dict.update({hash_key: (new_seq1, new_seq2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn\n",
      "[('f',), ('a', 'w'), ('n',)]\n",
      "[('F',), ('AO',), ('N',)]\n",
      "\n",
      "sermersheim\n",
      "[('s',), ('e', 'r'), ('m',), ('e', 'r'), ('s',), ('h',), ('e', 'i'), ('m',)]\n",
      "[('S',), ('ER',), ('M',), ('ER',), ('S',), ('HH',), ('AY',), ('M',)]\n",
      "\n",
      "sonji\n",
      "[('s',), ('o',), ('n',), ('j',), ('i',)]\n",
      "[('S',), ('AO',), ('N',), ('JH',), ('IY',)]\n",
      "\n",
      "scheuring\n",
      "[('s', 'c'), ('h',), ('e',), ('u', 'r'), ('i',), ('n', 'g')]\n",
      "[('SH',), (), (), ('ER',), ('IH',), ('NG',)]\n",
      "\n",
      "nunnery\n",
      "[('n',), ('u',), ('n', 'n'), ('e', 'r'), ('y',)]\n",
      "[('N',), ('AH',), ('N',), ('ER',), ('IY',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for key, val in grapheme_phoneme_alignment_dict.iteritems():\n",
    "    print key\n",
    "    print val[0]\n",
    "    print val[1]\n",
    "    print\n",
    "    c+=1\n",
    "    if c == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SH',), (), (), ('ER',), ('IH',), ('NG',)]\n",
      "[(0, 1), (1, 1), (1, 1), (1, 2), (2, 3), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "graph_chunks, phone_chunks = grapheme_phoneme_alignment_dict['scheuring']\n",
    "\n",
    "chunk_lengths = map(len, phone_chunks)\n",
    "valid_end_inds = np.cumsum(chunk_lengths)\n",
    "valid_start_inds = np.cumsum(chunk_lengths) - chunk_lengths\n",
    "\n",
    "print phone_chunks\n",
    "print zip(valid_start_inds, valid_end_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Map stress-less phones to stressed phones\n",
    "grapheme_to_stressed_phoneme_alignment_dict = {}\n",
    "for grapheme, (graph_chunks, phone_chunks) in grapheme_phoneme_alignment_dict.iteritems():\n",
    "    chunk_lengths = map(len, phone_chunks)\n",
    "    valid_end_inds = np.cumsum(chunk_lengths)\n",
    "    valid_start_inds = np.cumsum(chunk_lengths) - chunk_lengths\n",
    "    idx_pairs = zip(valid_start_inds,valid_end_inds)\n",
    "    \n",
    "    stressed_phoneme = cmu_dict[grapheme][0]\n",
    "    phone_chunks_stressed = [tuple(stressed_phoneme[start_idx:end_idx]) for (start_idx,end_idx) in idx_pairs]\n",
    "    grapheme_to_stressed_phoneme_alignment_dict.update({grapheme: (graph_chunks, phone_chunks_stressed)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn\n",
      "[('f',), ('a', 'w'), ('n',)]\n",
      "[(u'F',), (u'AO1',), (u'N',)]\n",
      "\n",
      "sermersheim\n",
      "[('s',), ('e', 'r'), ('m',), ('e', 'r'), ('s',), ('h',), ('e', 'i'), ('m',)]\n",
      "[(u'S',), (u'ER1',), (u'M',), (u'ER0',), (u'S',), (u'HH',), (u'AY0',), (u'M',)]\n",
      "\n",
      "sonji\n",
      "[('s',), ('o',), ('n',), ('j',), ('i',)]\n",
      "[(u'S',), (u'AO1',), (u'N',), (u'JH',), (u'IY0',)]\n",
      "\n",
      "scheuring\n",
      "[('s', 'c'), ('h',), ('e',), ('u', 'r'), ('i',), ('n', 'g')]\n",
      "[(u'SH',), (), (), (u'ER1',), (u'IH0',), (u'NG',)]\n",
      "\n",
      "nunnery\n",
      "[('n',), ('u',), ('n', 'n'), ('e', 'r'), ('y',)]\n",
      "[(u'N',), (u'AH1',), (u'N',), (u'ER0',), (u'IY0',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for key, val in grapheme_to_stressed_phoneme_alignment_dict.iteritems():\n",
    "    print key\n",
    "    print val[0]\n",
    "    print val[1]\n",
    "    print\n",
    "    c+=1\n",
    "    if c == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "import re\n",
    "from global_constants import ARPABET_PHONE_TO_PHONOLOGICAL_PHONE_DICT as a2p_dict\n",
    "\n",
    "# Build additional arpabet_phoneme to pronunciation_phoneme alignment_dict\n",
    "pattern = re.compile(\"^([a-z_\\-]+)+$\")\n",
    "\n",
    "# maps graphemes to 2-lists of (1) arpa-phone-tuples and (2) feature-phone-tuples\n",
    "arpabet_phoneme_to_feature_phoneme_alignment_dict = {}\n",
    "grapheme_to_feature_phoneme_dict = {}\n",
    "for grapheme, phoneme_list in cmu_dict.iteritems():\n",
    "    # don't need to downcase, because all graphemes in cmudict are already downcased\n",
    "    # only allow graphemes comprised of:\n",
    "    # 1) alpha\n",
    "    # 2) hyphen '-'\n",
    "    # 3) underscore '_'\n",
    "    if pattern.match(grapheme):\n",
    "        # comma notation is used to create length-1 tuples\n",
    "        arpabet_phone_chunks = [(phone,) for phone in phoneme_list[0]]\n",
    "        feature_phone_chunks = [tuple(a2p_dict[phone]) for phone in phoneme_list[0]]\n",
    "        feature_phoneme = sum(map(list, feature_phone_chunks), [])\n",
    "        \n",
    "        arpabet_phoneme_to_feature_phoneme_alignment_dict.update({grapheme: (arpabet_phone_chunks, feature_phone_chunks)})\n",
    "        grapheme_to_feature_phoneme_dict.update({grapheme: feature_phoneme})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "from pronunciation_dictionary import PronunciationDictionary\n",
    "\n",
    "pattern = re.compile(\"^([a-z_\\-]+)+$\")\n",
    "\n",
    "grapheme_to_stressed_phoneme_dict = {}\n",
    "for grapheme, phoneme_list in cmu_dict.iteritems():\n",
    "    # don't need to downcase, because all graphemes in cmudict are already downcased\n",
    "    # only allow graphemes comprised of:\n",
    "    # 1) alpha\n",
    "    # 2) hyphen '-'\n",
    "    # 3) underscore '_'\n",
    "    if pattern.match(grapheme):\n",
    "        grapheme_to_stressed_phoneme_dict.update({grapheme: phoneme_list[0]})\n",
    "            \n",
    "# grapheme_to_stressed_phoneme_pronunciation_dictionary = PronunciationDictionary(grapheme_to_stressed_phoneme_dict, grapheme_to_stressed_phoneme_alignment_dict)\n",
    "\n",
    "# grapheme_to_stressed_phoneme_pronunciation_dictionary.save('../data/grapheme_to_stressed_phoneme_pronunciation_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Word objects, and use it to initialize a PronunciationDictionary\n",
    "# Requires turning the 'alignment_dict' variables into proper SequenceAlignment objects\n",
    "# ...\n",
    "# BADLY need to clean up this script to make it comprehensible\n",
    "# Since it really does contain ALL of the preprocessing steps, should really make it a stand-alone .py file\n",
    "# which can be easily run from the terminal\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "from word import Word\n",
    "from pronunciation_dictionary import PronunciationDictionary\n",
    "from sequence_alignment import SequenceAlignment\n",
    "\n",
    "pattern = re.compile(\"^([a-z_\\-]+)+$\")\n",
    "\n",
    "word_list = []\n",
    "for grapheme, phoneme_list in cmu_dict.iteritems():\n",
    "    # don't need to downcase, because all words in cmudict are already downcased\n",
    "    # only allow words comprised of:\n",
    "    # 1) alpha\n",
    "    # 2) hyphen '-'\n",
    "    # 3) underscore '_'\n",
    "    if pattern.match(grapheme) and grapheme not in failed_words:\n",
    "        stressed_arpabet_phoneme = grapheme_to_stressed_phoneme_dict[grapheme]\n",
    "        graph_tuples, stressed_arpabet_phone_tuples = grapheme_to_stressed_phoneme_alignment_dict[grapheme]\n",
    "        grapheme_to_stressed_arpabet_phoneme_alignment = SequenceAlignment(graph_tuples, stressed_arpabet_phone_tuples)\n",
    "\n",
    "        feature_phoneme = grapheme_to_feature_phoneme_dict[grapheme]\n",
    "        arpabet_phone_tuples, feature_phone_tuples = arpabet_phoneme_to_feature_phoneme_alignment_dict[grapheme]\n",
    "        arpabet_phoneme_to_feature_phoneme_alignment = SequenceAlignment(arpabet_phone_tuples, feature_phone_tuples)\n",
    "        \n",
    "        new_word = Word(grapheme, stressed_arpabet_phoneme, feature_phoneme, grapheme_to_stressed_arpabet_phoneme_alignment, arpabet_phoneme_to_feature_phoneme_alignment)\n",
    "        word_list.append(new_word)\n",
    "\n",
    "PronunciationDictionary(word_list).save('../data/pronunciation_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
